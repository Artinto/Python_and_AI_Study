{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_HW_KJY.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-8hb_FSv9xI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49a0835-eab1-4113-bdae-1e41b1cca00d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMqG6XXDnrog"
      },
      "source": [
        "import torchvision\n",
        "import torch.utils.data as data\n",
        "from __future__ import print_function\n",
        "import argparse \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "trans = transforms.Compose([transforms.Resize((100,100)),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "                           ])\n",
        "trainset = torchvision.datasets.ImageFolder(root = \"/gdrive/MyDrive/custom_dataset/train\",\n",
        "                                           transform = trans)\n",
        "\n",
        "testset = torchvision.datasets.ImageFolder(root = \"/gdrive/MyDrive/custom_dataset/test\",\n",
        "                                           transform = trans)\n",
        "train_loader = data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = data.DataLoader(testset, batch_size=16, shuffle=True, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_lWv6a_rnc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acfa039-78ea-4baf-d690-7b44f2ce9d3c"
      },
      "source": [
        "class InceptionA(nn.Module): # 인셉션\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionA, self).__init__()\n",
        "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1) # (1x1)커널을 사용하여 in_channels에서 16개의 채널을 생성\n",
        "\n",
        "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1) # (1x1)커널을 사용하여 in_channels에서 16개의 채널을 생성\n",
        "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2) # (5x5)커널을 사용하여 16개의 채널에서 24개의 채널을 생성, 패딩은 바깥으로 2픽셀\n",
        "\n",
        "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1) # (1x1)커널을 사용하여 in_channels에서 16개의 채널을 생성\n",
        "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1) # (3x3)커널을 사용하여 16개의 채널에서 24개의 채널을 생성, 패딩은 바깥으로 1픽셀\n",
        "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1) # (3x3)커널을 사용하여 24개의 채널에서 24개의 채널을 생성, 패딩은 바깥으로 1픽셀\n",
        "\n",
        "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x) # Conv2d함수를 시행\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool] # 최종 아웃풋 리스트\n",
        "        return torch.cat(outputs, 1) # 텐서를 합침\n",
        "\n",
        "\n",
        "class Net(nn.Module): # nn.Module의 상속을 받는 신경망 클래스 Net 작성\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5) # 입력(색)은 3개, 출력은 10개, 커널(필터) 사이즈는 5x5\n",
        "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5) # 입력은 10개(conv1의 출력과 같음), 출력은 20개, 커널 사이즈는 5x5\n",
        "\n",
        "        self.incept1 = InceptionA(in_channels=10)\n",
        "        self.incept2 = InceptionA(in_channels=20)\n",
        "\n",
        "        self.mp = nn.MaxPool2d(2) # MaxPool 연산을 사용하여 최댓값만 뽑아냄\n",
        "        self.fc = nn.Linear(42592, 10)  # 입력 1408, 최종 출력 10의 선형함수\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x))) # 합성곱 신경망 conv1을 maxpool을 적용하고, relu함수로 돌림\n",
        "        x = self.incept1(x)\n",
        "        x = F.relu(self.mp(self.conv2(x))) # 합성곱 신경망 conv2을 maxpool을 적용하고, relu함수로 돌림\n",
        "        x = self.incept2(x)\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc(x) # 선형함수를 적용\n",
        "        return F.log_softmax(x) # NLLLoss를 사용하므로 마지막에 logsoftmax를 적용\n",
        "\n",
        "\n",
        "model = Net() # model에Net 클래스 적용\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.5) # 최적화 함수로 경사하강법을 실행, 학습률 0.01, 모멘텀(관성) 0.5로 설정\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader): # train_loader를 이용하여 각각의 배치사이즈의 data, target를 불러옴\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target) # NLL(Negative Log-Likehood)Loss를 사용하여 학습값과 참값을 비교함.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader: # test_loader를 이용하여 각각의 배치사이즈의 data, target를 불러옴\n",
        "        data, target = Variable(data, volatile=True), Variable(target) # volatile: 변수를 메모리에 저장한다.\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1] # 가장 큰 클래스의 인덱스값으로 예측한다.\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum() # 예측값과 타겟 데이터를 비교하여 얼마나 옳았는지 합을 계산\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset))) # 예측이 얼마나 맞았는지 Accuracy 출력\n",
        "\n",
        "\n",
        "for epoch in range(1, 30):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/1949 (0%)]\tLoss: 2.310014\n",
            "Train Epoch: 1 [160/1949 (8%)]\tLoss: 3.347622\n",
            "Train Epoch: 1 [320/1949 (16%)]\tLoss: 1.609471\n",
            "Train Epoch: 1 [480/1949 (25%)]\tLoss: 1.611371\n",
            "Train Epoch: 1 [640/1949 (33%)]\tLoss: 1.525377\n",
            "Train Epoch: 1 [800/1949 (41%)]\tLoss: 1.432281\n",
            "Train Epoch: 1 [960/1949 (49%)]\tLoss: 1.452495\n",
            "Train Epoch: 1 [1120/1949 (57%)]\tLoss: 1.335058\n",
            "Train Epoch: 1 [1280/1949 (66%)]\tLoss: 1.445786\n",
            "Train Epoch: 1 [1440/1949 (74%)]\tLoss: 1.559834\n",
            "Train Epoch: 1 [1600/1949 (82%)]\tLoss: 1.542935\n",
            "Train Epoch: 1 [1760/1949 (90%)]\tLoss: 1.567820\n",
            "Train Epoch: 1 [1920/1949 (98%)]\tLoss: 1.395973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 1.4126, Accuracy: 25/100 (25%)\n",
            "\n",
            "Train Epoch: 2 [0/1949 (0%)]\tLoss: 1.413544\n",
            "Train Epoch: 2 [160/1949 (8%)]\tLoss: 1.429758\n",
            "Train Epoch: 2 [320/1949 (16%)]\tLoss: 1.385036\n",
            "Train Epoch: 2 [480/1949 (25%)]\tLoss: 1.434894\n",
            "Train Epoch: 2 [640/1949 (33%)]\tLoss: 1.398673\n",
            "Train Epoch: 2 [800/1949 (41%)]\tLoss: 1.428791\n",
            "Train Epoch: 2 [960/1949 (49%)]\tLoss: 1.492878\n",
            "Train Epoch: 2 [1120/1949 (57%)]\tLoss: 1.354700\n",
            "Train Epoch: 2 [1280/1949 (66%)]\tLoss: 1.424479\n",
            "Train Epoch: 2 [1440/1949 (74%)]\tLoss: 1.512355\n",
            "Train Epoch: 2 [1600/1949 (82%)]\tLoss: 1.388198\n",
            "Train Epoch: 2 [1760/1949 (90%)]\tLoss: 1.503311\n",
            "Train Epoch: 2 [1920/1949 (98%)]\tLoss: 1.403635\n",
            "\n",
            "Test set: Average loss: 1.4033, Accuracy: 25/100 (25%)\n",
            "\n",
            "Train Epoch: 3 [0/1949 (0%)]\tLoss: 1.407338\n",
            "Train Epoch: 3 [160/1949 (8%)]\tLoss: 1.405027\n",
            "Train Epoch: 3 [320/1949 (16%)]\tLoss: 1.375847\n",
            "Train Epoch: 3 [480/1949 (25%)]\tLoss: 1.457075\n",
            "Train Epoch: 3 [640/1949 (33%)]\tLoss: 1.362470\n",
            "Train Epoch: 3 [800/1949 (41%)]\tLoss: 1.412740\n",
            "Train Epoch: 3 [960/1949 (49%)]\tLoss: 1.497405\n",
            "Train Epoch: 3 [1120/1949 (57%)]\tLoss: 1.374863\n",
            "Train Epoch: 3 [1280/1949 (66%)]\tLoss: 1.426128\n",
            "Train Epoch: 3 [1440/1949 (74%)]\tLoss: 1.463976\n",
            "Train Epoch: 3 [1600/1949 (82%)]\tLoss: 1.350154\n",
            "Train Epoch: 3 [1760/1949 (90%)]\tLoss: 1.432096\n",
            "Train Epoch: 3 [1920/1949 (98%)]\tLoss: 1.427625\n",
            "\n",
            "Test set: Average loss: 1.3998, Accuracy: 25/100 (25%)\n",
            "\n",
            "Train Epoch: 4 [0/1949 (0%)]\tLoss: 1.358375\n",
            "Train Epoch: 4 [160/1949 (8%)]\tLoss: 1.412161\n",
            "Train Epoch: 4 [320/1949 (16%)]\tLoss: 1.478226\n",
            "Train Epoch: 4 [480/1949 (25%)]\tLoss: 1.483462\n",
            "Train Epoch: 4 [640/1949 (33%)]\tLoss: 1.325842\n",
            "Train Epoch: 4 [800/1949 (41%)]\tLoss: 1.211679\n",
            "Train Epoch: 4 [960/1949 (49%)]\tLoss: 0.835451\n",
            "Train Epoch: 4 [1120/1949 (57%)]\tLoss: 1.055246\n",
            "Train Epoch: 4 [1280/1949 (66%)]\tLoss: 1.205818\n",
            "Train Epoch: 4 [1440/1949 (74%)]\tLoss: 1.251958\n",
            "Train Epoch: 4 [1600/1949 (82%)]\tLoss: 1.295116\n",
            "Train Epoch: 4 [1760/1949 (90%)]\tLoss: 0.726243\n",
            "Train Epoch: 4 [1920/1949 (98%)]\tLoss: 0.863476\n",
            "\n",
            "Test set: Average loss: 1.1083, Accuracy: 50/100 (50%)\n",
            "\n",
            "Train Epoch: 5 [0/1949 (0%)]\tLoss: 2.055980\n",
            "Train Epoch: 5 [160/1949 (8%)]\tLoss: 0.901971\n",
            "Train Epoch: 5 [320/1949 (16%)]\tLoss: 0.978712\n",
            "Train Epoch: 5 [480/1949 (25%)]\tLoss: 1.019322\n",
            "Train Epoch: 5 [640/1949 (33%)]\tLoss: 1.210838\n",
            "Train Epoch: 5 [800/1949 (41%)]\tLoss: 0.803006\n",
            "Train Epoch: 5 [960/1949 (49%)]\tLoss: 0.653417\n",
            "Train Epoch: 5 [1120/1949 (57%)]\tLoss: 0.878581\n",
            "Train Epoch: 5 [1280/1949 (66%)]\tLoss: 0.835814\n",
            "Train Epoch: 5 [1440/1949 (74%)]\tLoss: 0.938779\n",
            "Train Epoch: 5 [1600/1949 (82%)]\tLoss: 0.774838\n",
            "Train Epoch: 5 [1760/1949 (90%)]\tLoss: 1.335721\n",
            "Train Epoch: 5 [1920/1949 (98%)]\tLoss: 0.704168\n",
            "\n",
            "Test set: Average loss: 0.8421, Accuracy: 64/100 (64%)\n",
            "\n",
            "Train Epoch: 6 [0/1949 (0%)]\tLoss: 0.779537\n",
            "Train Epoch: 6 [160/1949 (8%)]\tLoss: 0.687380\n",
            "Train Epoch: 6 [320/1949 (16%)]\tLoss: 1.346781\n",
            "Train Epoch: 6 [480/1949 (25%)]\tLoss: 0.886792\n",
            "Train Epoch: 6 [640/1949 (33%)]\tLoss: 0.878380\n",
            "Train Epoch: 6 [800/1949 (41%)]\tLoss: 0.581209\n",
            "Train Epoch: 6 [960/1949 (49%)]\tLoss: 6.506649\n",
            "Train Epoch: 6 [1120/1949 (57%)]\tLoss: 2.241984\n",
            "Train Epoch: 6 [1280/1949 (66%)]\tLoss: 2.037878\n",
            "Train Epoch: 6 [1440/1949 (74%)]\tLoss: 1.032936\n",
            "Train Epoch: 6 [1600/1949 (82%)]\tLoss: 0.863656\n",
            "Train Epoch: 6 [1760/1949 (90%)]\tLoss: 1.497169\n",
            "Train Epoch: 6 [1920/1949 (98%)]\tLoss: 1.063445\n",
            "\n",
            "Test set: Average loss: 0.9719, Accuracy: 53/100 (53%)\n",
            "\n",
            "Train Epoch: 7 [0/1949 (0%)]\tLoss: 0.914656\n",
            "Train Epoch: 7 [160/1949 (8%)]\tLoss: 1.060800\n",
            "Train Epoch: 7 [320/1949 (16%)]\tLoss: 1.200053\n",
            "Train Epoch: 7 [480/1949 (25%)]\tLoss: 1.411747\n",
            "Train Epoch: 7 [640/1949 (33%)]\tLoss: 0.736665\n",
            "Train Epoch: 7 [800/1949 (41%)]\tLoss: 0.975918\n",
            "Train Epoch: 7 [960/1949 (49%)]\tLoss: 0.696359\n",
            "Train Epoch: 7 [1120/1949 (57%)]\tLoss: 0.863692\n",
            "Train Epoch: 7 [1280/1949 (66%)]\tLoss: 1.069817\n",
            "Train Epoch: 7 [1440/1949 (74%)]\tLoss: 0.810754\n",
            "Train Epoch: 7 [1600/1949 (82%)]\tLoss: 1.096173\n",
            "Train Epoch: 7 [1760/1949 (90%)]\tLoss: 0.832854\n",
            "Train Epoch: 7 [1920/1949 (98%)]\tLoss: 0.866681\n",
            "\n",
            "Test set: Average loss: 0.8959, Accuracy: 53/100 (53%)\n",
            "\n",
            "Train Epoch: 8 [0/1949 (0%)]\tLoss: 0.788531\n",
            "Train Epoch: 8 [160/1949 (8%)]\tLoss: 0.955268\n",
            "Train Epoch: 8 [320/1949 (16%)]\tLoss: 0.890679\n",
            "Train Epoch: 8 [480/1949 (25%)]\tLoss: 0.788987\n",
            "Train Epoch: 8 [640/1949 (33%)]\tLoss: 1.613287\n",
            "Train Epoch: 8 [800/1949 (41%)]\tLoss: 0.543733\n",
            "Train Epoch: 8 [960/1949 (49%)]\tLoss: 0.604526\n",
            "Train Epoch: 8 [1120/1949 (57%)]\tLoss: 0.746633\n",
            "Train Epoch: 8 [1280/1949 (66%)]\tLoss: 0.288568\n",
            "Train Epoch: 8 [1440/1949 (74%)]\tLoss: 1.061424\n",
            "Train Epoch: 8 [1600/1949 (82%)]\tLoss: 0.974845\n",
            "Train Epoch: 8 [1760/1949 (90%)]\tLoss: 1.374159\n",
            "Train Epoch: 8 [1920/1949 (98%)]\tLoss: 0.554740\n",
            "\n",
            "Test set: Average loss: 0.7852, Accuracy: 67/100 (67%)\n",
            "\n",
            "Train Epoch: 9 [0/1949 (0%)]\tLoss: 0.569090\n",
            "Train Epoch: 9 [160/1949 (8%)]\tLoss: 0.631171\n",
            "Train Epoch: 9 [320/1949 (16%)]\tLoss: 0.480484\n",
            "Train Epoch: 9 [480/1949 (25%)]\tLoss: 0.641815\n",
            "Train Epoch: 9 [640/1949 (33%)]\tLoss: 0.717969\n",
            "Train Epoch: 9 [800/1949 (41%)]\tLoss: 0.829038\n",
            "Train Epoch: 9 [960/1949 (49%)]\tLoss: 0.430871\n",
            "Train Epoch: 9 [1120/1949 (57%)]\tLoss: 1.501417\n",
            "Train Epoch: 9 [1280/1949 (66%)]\tLoss: 0.559170\n",
            "Train Epoch: 9 [1440/1949 (74%)]\tLoss: 0.302109\n",
            "Train Epoch: 9 [1600/1949 (82%)]\tLoss: 0.574753\n",
            "Train Epoch: 9 [1760/1949 (90%)]\tLoss: 0.409629\n",
            "Train Epoch: 9 [1920/1949 (98%)]\tLoss: 0.873797\n",
            "\n",
            "Test set: Average loss: 0.7546, Accuracy: 69/100 (69%)\n",
            "\n",
            "Train Epoch: 10 [0/1949 (0%)]\tLoss: 0.421285\n",
            "Train Epoch: 10 [160/1949 (8%)]\tLoss: 0.259857\n",
            "Train Epoch: 10 [320/1949 (16%)]\tLoss: 0.458251\n",
            "Train Epoch: 10 [480/1949 (25%)]\tLoss: 0.200326\n",
            "Train Epoch: 10 [640/1949 (33%)]\tLoss: 0.426036\n",
            "Train Epoch: 10 [800/1949 (41%)]\tLoss: 0.558110\n",
            "Train Epoch: 10 [960/1949 (49%)]\tLoss: 0.356270\n",
            "Train Epoch: 10 [1120/1949 (57%)]\tLoss: 0.395224\n",
            "Train Epoch: 10 [1280/1949 (66%)]\tLoss: 0.455288\n",
            "Train Epoch: 10 [1440/1949 (74%)]\tLoss: 0.339553\n",
            "Train Epoch: 10 [1600/1949 (82%)]\tLoss: 0.643561\n",
            "Train Epoch: 10 [1760/1949 (90%)]\tLoss: 0.544416\n",
            "Train Epoch: 10 [1920/1949 (98%)]\tLoss: 0.354817\n",
            "\n",
            "Test set: Average loss: 0.5333, Accuracy: 77/100 (77%)\n",
            "\n",
            "Train Epoch: 11 [0/1949 (0%)]\tLoss: 0.483893\n",
            "Train Epoch: 11 [160/1949 (8%)]\tLoss: 0.556238\n",
            "Train Epoch: 11 [320/1949 (16%)]\tLoss: 0.199502\n",
            "Train Epoch: 11 [480/1949 (25%)]\tLoss: 0.196491\n",
            "Train Epoch: 11 [640/1949 (33%)]\tLoss: 0.355724\n",
            "Train Epoch: 11 [800/1949 (41%)]\tLoss: 0.289895\n",
            "Train Epoch: 11 [960/1949 (49%)]\tLoss: 0.163952\n",
            "Train Epoch: 11 [1120/1949 (57%)]\tLoss: 0.284786\n",
            "Train Epoch: 11 [1280/1949 (66%)]\tLoss: 0.409433\n",
            "Train Epoch: 11 [1440/1949 (74%)]\tLoss: 0.362109\n",
            "Train Epoch: 11 [1600/1949 (82%)]\tLoss: 0.438406\n",
            "Train Epoch: 11 [1760/1949 (90%)]\tLoss: 0.111520\n",
            "Train Epoch: 11 [1920/1949 (98%)]\tLoss: 0.755782\n",
            "\n",
            "Test set: Average loss: 0.6074, Accuracy: 74/100 (74%)\n",
            "\n",
            "Train Epoch: 12 [0/1949 (0%)]\tLoss: 0.313132\n",
            "Train Epoch: 12 [160/1949 (8%)]\tLoss: 0.319115\n",
            "Train Epoch: 12 [320/1949 (16%)]\tLoss: 0.204058\n",
            "Train Epoch: 12 [480/1949 (25%)]\tLoss: 0.565843\n",
            "Train Epoch: 12 [640/1949 (33%)]\tLoss: 0.269217\n",
            "Train Epoch: 12 [800/1949 (41%)]\tLoss: 0.091425\n",
            "Train Epoch: 12 [960/1949 (49%)]\tLoss: 0.324779\n",
            "Train Epoch: 12 [1120/1949 (57%)]\tLoss: 0.356991\n",
            "Train Epoch: 12 [1280/1949 (66%)]\tLoss: 0.303728\n",
            "Train Epoch: 12 [1440/1949 (74%)]\tLoss: 0.097183\n",
            "Train Epoch: 12 [1600/1949 (82%)]\tLoss: 0.464854\n",
            "Train Epoch: 12 [1760/1949 (90%)]\tLoss: 0.569512\n",
            "Train Epoch: 12 [1920/1949 (98%)]\tLoss: 1.352149\n",
            "\n",
            "Test set: Average loss: 1.0438, Accuracy: 64/100 (64%)\n",
            "\n",
            "Train Epoch: 13 [0/1949 (0%)]\tLoss: 1.265226\n",
            "Train Epoch: 13 [160/1949 (8%)]\tLoss: 0.428356\n",
            "Train Epoch: 13 [320/1949 (16%)]\tLoss: 0.136346\n",
            "Train Epoch: 13 [480/1949 (25%)]\tLoss: 0.153605\n",
            "Train Epoch: 13 [640/1949 (33%)]\tLoss: 0.254060\n",
            "Train Epoch: 13 [800/1949 (41%)]\tLoss: 1.017874\n",
            "Train Epoch: 13 [960/1949 (49%)]\tLoss: 0.858905\n",
            "Train Epoch: 13 [1120/1949 (57%)]\tLoss: 0.196955\n",
            "Train Epoch: 13 [1280/1949 (66%)]\tLoss: 0.399751\n",
            "Train Epoch: 13 [1440/1949 (74%)]\tLoss: 0.418489\n",
            "Train Epoch: 13 [1600/1949 (82%)]\tLoss: 0.253046\n",
            "Train Epoch: 13 [1760/1949 (90%)]\tLoss: 0.601848\n",
            "Train Epoch: 13 [1920/1949 (98%)]\tLoss: 0.114266\n",
            "\n",
            "Test set: Average loss: 0.5487, Accuracy: 78/100 (78%)\n",
            "\n",
            "Train Epoch: 14 [0/1949 (0%)]\tLoss: 0.440416\n",
            "Train Epoch: 14 [160/1949 (8%)]\tLoss: 0.102267\n",
            "Train Epoch: 14 [320/1949 (16%)]\tLoss: 0.248819\n",
            "Train Epoch: 14 [480/1949 (25%)]\tLoss: 0.138196\n",
            "Train Epoch: 14 [640/1949 (33%)]\tLoss: 0.146337\n",
            "Train Epoch: 14 [800/1949 (41%)]\tLoss: 0.133352\n",
            "Train Epoch: 14 [960/1949 (49%)]\tLoss: 0.174537\n",
            "Train Epoch: 14 [1120/1949 (57%)]\tLoss: 0.684630\n",
            "Train Epoch: 14 [1280/1949 (66%)]\tLoss: 0.484083\n",
            "Train Epoch: 14 [1440/1949 (74%)]\tLoss: 0.378067\n",
            "Train Epoch: 14 [1600/1949 (82%)]\tLoss: 0.405328\n",
            "Train Epoch: 14 [1760/1949 (90%)]\tLoss: 0.473539\n",
            "Train Epoch: 14 [1920/1949 (98%)]\tLoss: 0.332039\n",
            "\n",
            "Test set: Average loss: 0.5952, Accuracy: 79/100 (79%)\n",
            "\n",
            "Train Epoch: 15 [0/1949 (0%)]\tLoss: 0.159968\n",
            "Train Epoch: 15 [160/1949 (8%)]\tLoss: 0.216615\n",
            "Train Epoch: 15 [320/1949 (16%)]\tLoss: 0.070143\n",
            "Train Epoch: 15 [480/1949 (25%)]\tLoss: 0.308351\n",
            "Train Epoch: 15 [640/1949 (33%)]\tLoss: 0.155591\n",
            "Train Epoch: 15 [800/1949 (41%)]\tLoss: 0.316839\n",
            "Train Epoch: 15 [960/1949 (49%)]\tLoss: 0.786718\n",
            "Train Epoch: 15 [1120/1949 (57%)]\tLoss: 0.211858\n",
            "Train Epoch: 15 [1280/1949 (66%)]\tLoss: 0.238024\n",
            "Train Epoch: 15 [1440/1949 (74%)]\tLoss: 0.198411\n",
            "Train Epoch: 15 [1600/1949 (82%)]\tLoss: 0.093969\n",
            "Train Epoch: 15 [1760/1949 (90%)]\tLoss: 0.438702\n",
            "Train Epoch: 15 [1920/1949 (98%)]\tLoss: 0.976418\n",
            "\n",
            "Test set: Average loss: 0.5334, Accuracy: 86/100 (86%)\n",
            "\n",
            "Train Epoch: 16 [0/1949 (0%)]\tLoss: 0.304947\n",
            "Train Epoch: 16 [160/1949 (8%)]\tLoss: 0.109778\n",
            "Train Epoch: 16 [320/1949 (16%)]\tLoss: 0.048879\n",
            "Train Epoch: 16 [480/1949 (25%)]\tLoss: 0.131320\n",
            "Train Epoch: 16 [640/1949 (33%)]\tLoss: 0.138661\n",
            "Train Epoch: 16 [800/1949 (41%)]\tLoss: 0.112866\n",
            "Train Epoch: 16 [960/1949 (49%)]\tLoss: 0.223864\n",
            "Train Epoch: 16 [1120/1949 (57%)]\tLoss: 0.006004\n",
            "Train Epoch: 16 [1280/1949 (66%)]\tLoss: 0.247647\n",
            "Train Epoch: 16 [1440/1949 (74%)]\tLoss: 0.106049\n",
            "Train Epoch: 16 [1600/1949 (82%)]\tLoss: 0.078617\n",
            "Train Epoch: 16 [1760/1949 (90%)]\tLoss: 0.103346\n",
            "Train Epoch: 16 [1920/1949 (98%)]\tLoss: 0.222864\n",
            "\n",
            "Test set: Average loss: 0.5052, Accuracy: 86/100 (86%)\n",
            "\n",
            "Train Epoch: 17 [0/1949 (0%)]\tLoss: 0.017275\n",
            "Train Epoch: 17 [160/1949 (8%)]\tLoss: 0.138684\n",
            "Train Epoch: 17 [320/1949 (16%)]\tLoss: 0.082519\n",
            "Train Epoch: 17 [480/1949 (25%)]\tLoss: 0.139633\n",
            "Train Epoch: 17 [640/1949 (33%)]\tLoss: 0.511338\n",
            "Train Epoch: 17 [800/1949 (41%)]\tLoss: 0.639882\n",
            "Train Epoch: 17 [960/1949 (49%)]\tLoss: 0.080541\n",
            "Train Epoch: 17 [1120/1949 (57%)]\tLoss: 0.051127\n",
            "Train Epoch: 17 [1280/1949 (66%)]\tLoss: 0.662167\n",
            "Train Epoch: 17 [1440/1949 (74%)]\tLoss: 0.199439\n",
            "Train Epoch: 17 [1600/1949 (82%)]\tLoss: 0.460047\n",
            "Train Epoch: 17 [1760/1949 (90%)]\tLoss: 0.190613\n",
            "Train Epoch: 17 [1920/1949 (98%)]\tLoss: 0.041189\n",
            "\n",
            "Test set: Average loss: 0.4204, Accuracy: 89/100 (89%)\n",
            "\n",
            "Train Epoch: 18 [0/1949 (0%)]\tLoss: 0.056907\n",
            "Train Epoch: 18 [160/1949 (8%)]\tLoss: 0.243477\n",
            "Train Epoch: 18 [320/1949 (16%)]\tLoss: 0.023974\n",
            "Train Epoch: 18 [480/1949 (25%)]\tLoss: 0.020753\n",
            "Train Epoch: 18 [640/1949 (33%)]\tLoss: 0.014706\n",
            "Train Epoch: 18 [800/1949 (41%)]\tLoss: 0.027928\n",
            "Train Epoch: 18 [960/1949 (49%)]\tLoss: 0.045333\n",
            "Train Epoch: 18 [1120/1949 (57%)]\tLoss: 0.072627\n",
            "Train Epoch: 18 [1280/1949 (66%)]\tLoss: 0.005245\n",
            "Train Epoch: 18 [1440/1949 (74%)]\tLoss: 0.008610\n",
            "Train Epoch: 18 [1600/1949 (82%)]\tLoss: 0.039527\n",
            "Train Epoch: 18 [1760/1949 (90%)]\tLoss: 0.097797\n",
            "Train Epoch: 18 [1920/1949 (98%)]\tLoss: 0.118211\n",
            "\n",
            "Test set: Average loss: 0.8038, Accuracy: 80/100 (80%)\n",
            "\n",
            "Train Epoch: 19 [0/1949 (0%)]\tLoss: 0.050311\n",
            "Train Epoch: 19 [160/1949 (8%)]\tLoss: 0.029284\n",
            "Train Epoch: 19 [320/1949 (16%)]\tLoss: 0.456251\n",
            "Train Epoch: 19 [480/1949 (25%)]\tLoss: 0.177866\n",
            "Train Epoch: 19 [640/1949 (33%)]\tLoss: 0.283467\n",
            "Train Epoch: 19 [800/1949 (41%)]\tLoss: 0.005600\n",
            "Train Epoch: 19 [960/1949 (49%)]\tLoss: 0.079271\n",
            "Train Epoch: 19 [1120/1949 (57%)]\tLoss: 0.033099\n",
            "Train Epoch: 19 [1280/1949 (66%)]\tLoss: 0.210425\n",
            "Train Epoch: 19 [1440/1949 (74%)]\tLoss: 0.157343\n",
            "Train Epoch: 19 [1600/1949 (82%)]\tLoss: 0.069437\n",
            "Train Epoch: 19 [1760/1949 (90%)]\tLoss: 0.155174\n",
            "Train Epoch: 19 [1920/1949 (98%)]\tLoss: 0.132740\n",
            "\n",
            "Test set: Average loss: 0.6743, Accuracy: 83/100 (83%)\n",
            "\n",
            "Train Epoch: 20 [0/1949 (0%)]\tLoss: 0.137148\n",
            "Train Epoch: 20 [160/1949 (8%)]\tLoss: 0.041895\n",
            "Train Epoch: 20 [320/1949 (16%)]\tLoss: 0.008466\n",
            "Train Epoch: 20 [480/1949 (25%)]\tLoss: 0.063227\n",
            "Train Epoch: 20 [640/1949 (33%)]\tLoss: 0.027129\n",
            "Train Epoch: 20 [800/1949 (41%)]\tLoss: 0.273533\n",
            "Train Epoch: 20 [960/1949 (49%)]\tLoss: 0.014090\n",
            "Train Epoch: 20 [1120/1949 (57%)]\tLoss: 0.018513\n",
            "Train Epoch: 20 [1280/1949 (66%)]\tLoss: 0.015271\n",
            "Train Epoch: 20 [1440/1949 (74%)]\tLoss: 0.051063\n",
            "Train Epoch: 20 [1600/1949 (82%)]\tLoss: 0.248551\n",
            "Train Epoch: 20 [1760/1949 (90%)]\tLoss: 0.417637\n",
            "Train Epoch: 20 [1920/1949 (98%)]\tLoss: 0.037969\n",
            "\n",
            "Test set: Average loss: 0.6692, Accuracy: 81/100 (81%)\n",
            "\n",
            "Train Epoch: 21 [0/1949 (0%)]\tLoss: 0.024059\n",
            "Train Epoch: 21 [160/1949 (8%)]\tLoss: 0.006542\n",
            "Train Epoch: 21 [320/1949 (16%)]\tLoss: 0.015467\n",
            "Train Epoch: 21 [480/1949 (25%)]\tLoss: 0.025229\n",
            "Train Epoch: 21 [640/1949 (33%)]\tLoss: 0.008343\n",
            "Train Epoch: 21 [800/1949 (41%)]\tLoss: 0.363012\n",
            "Train Epoch: 21 [960/1949 (49%)]\tLoss: 0.009664\n",
            "Train Epoch: 21 [1120/1949 (57%)]\tLoss: 0.189191\n",
            "Train Epoch: 21 [1280/1949 (66%)]\tLoss: 0.026507\n",
            "Train Epoch: 21 [1440/1949 (74%)]\tLoss: 0.034602\n",
            "Train Epoch: 21 [1600/1949 (82%)]\tLoss: 0.606107\n",
            "Train Epoch: 21 [1760/1949 (90%)]\tLoss: 0.010322\n",
            "Train Epoch: 21 [1920/1949 (98%)]\tLoss: 0.033234\n",
            "\n",
            "Test set: Average loss: 0.7404, Accuracy: 82/100 (82%)\n",
            "\n",
            "Train Epoch: 22 [0/1949 (0%)]\tLoss: 0.158339\n",
            "Train Epoch: 22 [160/1949 (8%)]\tLoss: 0.007943\n",
            "Train Epoch: 22 [320/1949 (16%)]\tLoss: 0.030265\n",
            "Train Epoch: 22 [480/1949 (25%)]\tLoss: 0.005844\n",
            "Train Epoch: 22 [640/1949 (33%)]\tLoss: 0.034695\n",
            "Train Epoch: 22 [800/1949 (41%)]\tLoss: 0.082956\n",
            "Train Epoch: 22 [960/1949 (49%)]\tLoss: 0.137895\n",
            "Train Epoch: 22 [1120/1949 (57%)]\tLoss: 0.002873\n",
            "Train Epoch: 22 [1280/1949 (66%)]\tLoss: 0.000940\n",
            "Train Epoch: 22 [1440/1949 (74%)]\tLoss: 0.016893\n",
            "Train Epoch: 22 [1600/1949 (82%)]\tLoss: 0.003834\n",
            "Train Epoch: 22 [1760/1949 (90%)]\tLoss: 0.004214\n",
            "Train Epoch: 22 [1920/1949 (98%)]\tLoss: 0.045319\n",
            "\n",
            "Test set: Average loss: 0.7764, Accuracy: 85/100 (85%)\n",
            "\n",
            "Train Epoch: 23 [0/1949 (0%)]\tLoss: 0.001922\n",
            "Train Epoch: 23 [160/1949 (8%)]\tLoss: 0.000898\n",
            "Train Epoch: 23 [320/1949 (16%)]\tLoss: 0.098875\n",
            "Train Epoch: 23 [480/1949 (25%)]\tLoss: 0.001246\n",
            "Train Epoch: 23 [640/1949 (33%)]\tLoss: 0.001350\n",
            "Train Epoch: 23 [800/1949 (41%)]\tLoss: 0.000559\n",
            "Train Epoch: 23 [960/1949 (49%)]\tLoss: 0.000366\n",
            "Train Epoch: 23 [1120/1949 (57%)]\tLoss: 0.020490\n",
            "Train Epoch: 23 [1280/1949 (66%)]\tLoss: 0.026168\n",
            "Train Epoch: 23 [1440/1949 (74%)]\tLoss: 0.024421\n",
            "Train Epoch: 23 [1600/1949 (82%)]\tLoss: 0.001934\n",
            "Train Epoch: 23 [1760/1949 (90%)]\tLoss: 0.003156\n",
            "Train Epoch: 23 [1920/1949 (98%)]\tLoss: 0.022653\n",
            "\n",
            "Test set: Average loss: 0.8223, Accuracy: 86/100 (86%)\n",
            "\n",
            "Train Epoch: 24 [0/1949 (0%)]\tLoss: 0.001206\n",
            "Train Epoch: 24 [160/1949 (8%)]\tLoss: 0.000628\n",
            "Train Epoch: 24 [320/1949 (16%)]\tLoss: 0.002943\n",
            "Train Epoch: 24 [480/1949 (25%)]\tLoss: 0.012796\n",
            "Train Epoch: 24 [640/1949 (33%)]\tLoss: 0.000329\n",
            "Train Epoch: 24 [800/1949 (41%)]\tLoss: 0.001263\n",
            "Train Epoch: 24 [960/1949 (49%)]\tLoss: 0.001229\n",
            "Train Epoch: 24 [1120/1949 (57%)]\tLoss: 0.000874\n",
            "Train Epoch: 24 [1280/1949 (66%)]\tLoss: 0.000199\n",
            "Train Epoch: 24 [1440/1949 (74%)]\tLoss: 0.001103\n",
            "Train Epoch: 24 [1600/1949 (82%)]\tLoss: 0.000862\n",
            "Train Epoch: 24 [1760/1949 (90%)]\tLoss: 0.001857\n",
            "Train Epoch: 24 [1920/1949 (98%)]\tLoss: 0.004927\n",
            "\n",
            "Test set: Average loss: 0.8743, Accuracy: 85/100 (85%)\n",
            "\n",
            "Train Epoch: 25 [0/1949 (0%)]\tLoss: 0.003025\n",
            "Train Epoch: 25 [160/1949 (8%)]\tLoss: 0.001327\n",
            "Train Epoch: 25 [320/1949 (16%)]\tLoss: 0.000936\n",
            "Train Epoch: 25 [480/1949 (25%)]\tLoss: 0.000568\n",
            "Train Epoch: 25 [640/1949 (33%)]\tLoss: 0.001530\n",
            "Train Epoch: 25 [800/1949 (41%)]\tLoss: 0.001104\n",
            "Train Epoch: 25 [960/1949 (49%)]\tLoss: 0.000177\n",
            "Train Epoch: 25 [1120/1949 (57%)]\tLoss: 0.001132\n",
            "Train Epoch: 25 [1280/1949 (66%)]\tLoss: 0.000784\n",
            "Train Epoch: 25 [1440/1949 (74%)]\tLoss: 0.000962\n",
            "Train Epoch: 25 [1600/1949 (82%)]\tLoss: 0.001148\n",
            "Train Epoch: 25 [1760/1949 (90%)]\tLoss: 0.001024\n",
            "Train Epoch: 25 [1920/1949 (98%)]\tLoss: 0.000797\n",
            "\n",
            "Test set: Average loss: 0.8382, Accuracy: 85/100 (85%)\n",
            "\n",
            "Train Epoch: 26 [0/1949 (0%)]\tLoss: 0.000556\n",
            "Train Epoch: 26 [160/1949 (8%)]\tLoss: 0.000973\n",
            "Train Epoch: 26 [320/1949 (16%)]\tLoss: 0.000256\n",
            "Train Epoch: 26 [480/1949 (25%)]\tLoss: 0.001019\n",
            "Train Epoch: 26 [640/1949 (33%)]\tLoss: 0.000658\n",
            "Train Epoch: 26 [800/1949 (41%)]\tLoss: 0.000750\n",
            "Train Epoch: 26 [960/1949 (49%)]\tLoss: 0.000647\n",
            "Train Epoch: 26 [1120/1949 (57%)]\tLoss: 0.000719\n",
            "Train Epoch: 26 [1280/1949 (66%)]\tLoss: 0.000392\n",
            "Train Epoch: 26 [1440/1949 (74%)]\tLoss: 0.001303\n",
            "Train Epoch: 26 [1600/1949 (82%)]\tLoss: 0.002497\n",
            "Train Epoch: 26 [1760/1949 (90%)]\tLoss: 0.001559\n",
            "Train Epoch: 26 [1920/1949 (98%)]\tLoss: 0.000145\n",
            "\n",
            "Test set: Average loss: 0.8576, Accuracy: 86/100 (86%)\n",
            "\n",
            "Train Epoch: 27 [0/1949 (0%)]\tLoss: 0.000645\n",
            "Train Epoch: 27 [160/1949 (8%)]\tLoss: 0.000941\n",
            "Train Epoch: 27 [320/1949 (16%)]\tLoss: 0.000395\n",
            "Train Epoch: 27 [480/1949 (25%)]\tLoss: 0.000033\n",
            "Train Epoch: 27 [640/1949 (33%)]\tLoss: 0.000097\n",
            "Train Epoch: 27 [800/1949 (41%)]\tLoss: 0.000555\n",
            "Train Epoch: 27 [960/1949 (49%)]\tLoss: 0.000376\n",
            "Train Epoch: 27 [1120/1949 (57%)]\tLoss: 0.000133\n",
            "Train Epoch: 27 [1280/1949 (66%)]\tLoss: 0.000360\n",
            "Train Epoch: 27 [1440/1949 (74%)]\tLoss: 0.000802\n",
            "Train Epoch: 27 [1600/1949 (82%)]\tLoss: 0.000216\n",
            "Train Epoch: 27 [1760/1949 (90%)]\tLoss: 0.000802\n",
            "Train Epoch: 27 [1920/1949 (98%)]\tLoss: 0.000198\n",
            "\n",
            "Test set: Average loss: 0.8651, Accuracy: 86/100 (86%)\n",
            "\n",
            "Train Epoch: 28 [0/1949 (0%)]\tLoss: 0.000180\n",
            "Train Epoch: 28 [160/1949 (8%)]\tLoss: 0.000137\n",
            "Train Epoch: 28 [320/1949 (16%)]\tLoss: 0.000422\n",
            "Train Epoch: 28 [480/1949 (25%)]\tLoss: 0.000464\n",
            "Train Epoch: 28 [640/1949 (33%)]\tLoss: 0.000021\n",
            "Train Epoch: 28 [800/1949 (41%)]\tLoss: 0.000195\n",
            "Train Epoch: 28 [960/1949 (49%)]\tLoss: 0.000494\n",
            "Train Epoch: 28 [1120/1949 (57%)]\tLoss: 0.001118\n",
            "Train Epoch: 28 [1280/1949 (66%)]\tLoss: 0.000564\n",
            "Train Epoch: 28 [1440/1949 (74%)]\tLoss: 0.000025\n",
            "Train Epoch: 28 [1600/1949 (82%)]\tLoss: 0.000067\n",
            "Train Epoch: 28 [1760/1949 (90%)]\tLoss: 0.001482\n",
            "Train Epoch: 28 [1920/1949 (98%)]\tLoss: 0.000644\n",
            "\n",
            "Test set: Average loss: 0.8778, Accuracy: 86/100 (86%)\n",
            "\n",
            "Train Epoch: 29 [0/1949 (0%)]\tLoss: 0.000482\n",
            "Train Epoch: 29 [160/1949 (8%)]\tLoss: 0.000338\n",
            "Train Epoch: 29 [320/1949 (16%)]\tLoss: 0.001644\n",
            "Train Epoch: 29 [480/1949 (25%)]\tLoss: 0.000479\n",
            "Train Epoch: 29 [640/1949 (33%)]\tLoss: 0.001496\n",
            "Train Epoch: 29 [800/1949 (41%)]\tLoss: 0.000081\n",
            "Train Epoch: 29 [960/1949 (49%)]\tLoss: 0.001011\n",
            "Train Epoch: 29 [1120/1949 (57%)]\tLoss: 0.000030\n",
            "Train Epoch: 29 [1280/1949 (66%)]\tLoss: 0.001531\n",
            "Train Epoch: 29 [1440/1949 (74%)]\tLoss: 0.000952\n",
            "Train Epoch: 29 [1600/1949 (82%)]\tLoss: 0.000579\n",
            "Train Epoch: 29 [1760/1949 (90%)]\tLoss: 0.000866\n",
            "Train Epoch: 29 [1920/1949 (98%)]\tLoss: 0.000861\n",
            "\n",
            "Test set: Average loss: 0.8869, Accuracy: 86/100 (86%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgyQW1CwJKCQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}