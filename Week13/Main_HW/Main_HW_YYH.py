# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NYQZxYj-rcYWPOwZuraqYfBYOXdpn5jv
"""

# Commented out IPython magic to ensure Python compatibility.

from PIL import Image
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import torch
import numpy as np
import torchvision
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from torchvision.datasets import ImageFolder
import torch.utils.data as data
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torch.nn.functional as F

trainset = ImageFolder("/content/drive/MyDrive/custom_dataset/train",
                         transform=transforms.Compose([transforms.RandomCrop(100),
                                                       transforms.ToTensor()]))

testset = ImageFolder("/content/drive/MyDrive/custom_dataset/test",
                        transform=transforms.Compose([transforms.RandomCrop(100),
                                                      transforms.ToTensor()]))

train_loader = data.DataLoader(trainset, batch_size=32, shuffle=True)
test_loader = data.DataLoader(testset, batch_size=32, shuffle=True)


class InceptionA(nn.Module):

    def __init__(self, in_channels):
        super(InceptionA, self).__init__()
        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)      # 1x1 필터로 인 채널 -> 아웃 16으로 만든다

        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)    # 1x1 필터로 인 채널 -> 아웃 16으로 만든다
        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)  # 5x5 필터로 인 16 -> 아웃 24으로 만든다 padding=2 로 함으로써 사이즈를 맞춤
        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)
        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1) #사이즈를 맞추기위해 2번 진행한것으로 생각됌

        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1) #2d avg 풀링링  
        branch_pool = self.branch_pool(branch_pool)                       

        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool] 
        return torch.cat(outputs, 1)#텐서들을 모두 합침   


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)    #인 1채널 -> 아웃 16으로 만든다
        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)   #인 88채널 -> 아웃 20으로 만든다

        self.incept1 = InceptionA(in_channels=10)     #인셉션에 in_channel 입력
        self.incept2 = InceptionA(in_channels=20)

        self.mp = nn.MaxPool2d(2)                     #maxpooling 으로 사이즈 절반
        self.fc = nn.Linear(42592, 10)         

    def forward(self, x):
        in_size = x.size(0)
        x = F.relu(self.mp(self.conv1(x)))
        x = self.incept1(x)
        x = F.relu(self.mp(self.conv2(x)))
        x = self.incept2(x)
        x = x.view(in_size, -1)  # flatten the tensor
        x = self.fc(x)
        return F.log_softmax(x,dim=0)


model = Net()

optimizer = optim.SGD(model.parameters(), lr=0.015, momentum=0.6)


def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 10 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data))


def test():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        # sum up batch loss
        test_loss += F.nll_loss(output, target, reduction='sum').data
        # get the index of the max log-probability
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


for epoch in range(1, 10):
    train(epoch)
    test()