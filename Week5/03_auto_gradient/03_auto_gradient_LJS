import torch # Deep learning Framework
import pdb  # Debug tool

x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

# Weight 변수를 담기 위한 Tensor 생성 + Gradient 계산
# w라는 텐서에 1.0 값을 할당하여 초기화 
w = torch.tensor([1.0], requires_grad=True)

# Forward pass
def forward(x): 
    return x * w

# Loss function
def loss(y_pred, y_val):
    return (y_pred - y_val)*(y_pred - y_val)

print("Prediction (before training)",  4, forward(4).item()) 

# Training loop
# Weight Updata : Forward - Backward 1회 반복 후 Update
for epoch in range(10):
    for x_val, y_val in zip(x_data, y_data):
        y_pred = forward(x_val) # Forward pass
        l = loss(y_pred, y_val) # loss 계산
        l.backward()            # loss와 각 텐서의 Gradient를 이용하여 Backpropagation 진행 
        
        # item : dictionary 형태로 반환
        # 입력 데이터와 출력 데이터를 출력하고, 그에 대한 Gradient 출력  
        print("\tgrad: ", x_val, y_val, w.grad.item())
        
        # Back-propagation으로 구한 Weight를 Manually Updating 
        w.data = w.data - 0.01 * w.grad.item()
        
        # 새 데이터를 위해 Gradient 데이터 초기화
        w.grad.data.zero_()

    print(f"Epoch: {epoch} | Loss: {l.item()}")
    
# After training
print("Prediction (after training)",  4, forward(4).item())
