{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각종 모듈 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn : neural network 에 필요한 각종 함수 *CrossEntropyLoss 등 포함\n",
    "# tensor : array를 tensor type 로 바꿔줌\n",
    "# max : 주어진 텐서 배열의 최대 값이 들어있는 index를 리턴하는 함수\n",
    "from torch import nn, tensor, max \n",
    "\n",
    "# numpy : array 만들 때 사용\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cross entropy 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy example\n",
    "# One hot\n",
    "# 0: 1 0 0\n",
    "# 1: 0 1 0\n",
    "# 2: 0 0 1\n",
    "# y 실제값\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "# y 예측값 1,2\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1]) # 최댓값 0.7의 index 가 y 실제값의 최댓값 index와 일치 -> right\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6]) # 최댓값 0.6의 index 가 y 실제값 최댓값 index와 아예 다름 -> wrong\n",
    "\n",
    "# -y*log(y_pred) 통해 오차 구함 --> 극적인 차이가 발생하지는 않는다.\n",
    "print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}') # 출력 값 0.35 -> 낮은 오차\n",
    "print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}') # 출력 값 2.3 -> 높은 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Softmax + CrossEntropy (LogSoftmax + NLLLoss) Tensor 행 1개 버전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogSoftmax \n",
    "LogSoftmax(xi) = log( exp(xi)  / ∑ j exp(xj) )\n",
    "-> 특정 행에서 i번째 열의 exp(xi)값 / 특정 행의 모든 값에 exp취한 것의 합계\n",
    "-> 나중에 손실함수에서 첫번째 argument 로 들어갈 y_pred 값에 적용\n",
    "\n",
    "### NLLLoss (negative log likelihood loss)\n",
    "-> 나중에 손실함수에서 \n",
    "-> 첫번째 argument 로 들어갈 y_pred 값을 받음\n",
    "-> 두번째 argument 로 들어갈 target : class 값인 y 값을 받아서 원 핫 인코딩\n",
    "-> \n",
    "-ylog(y_pred) 연산 해줌(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  target y 초기화 - [0] : 행 1개 짜리 [1,0,0] 원핫을 class 로 사용할 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = tensor([0], requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y_pred 선언 (logit 선언)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred2 = tensor([[0.5, 2.0, 0.3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_pred 는 input softmax 적용이 안된 logit 단계\n",
    "# Y 는 class\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}') # 0.41, 1.84\n",
    "print(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}') \n",
    "print(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Softmax + CrossEntropy (LogSoftmax + NLLLoss) Tensor 미니배치버전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  target y 초기화 - [2,0,1] : 행 3개 짜리 [[0,0,1],[1,0,0],[0,1,0]] 원핫을 class 로 사용할 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = tensor([2, 0, 1], requires_grad=False) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y_pred 미니배치 선언 (logit 선언)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = tensor([[0.1, 0.2, 0.9],\n",
    "                  [1.1, 0.1, 0.2],\n",
    "                  [0.2, 2.1, 0.1]])\n",
    "\n",
    "Y_pred2 = tensor([[0.8, 0.2, 0.3],\n",
    "                  [0.2, 0.3, 0.5],\n",
    "                  [0.2, 0.2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "print(f'Batch Loss1:  {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}') # 0.5, 1.24"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
